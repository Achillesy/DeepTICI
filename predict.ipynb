{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Achillesy/Thrombolysis_DeepTICI/blob/main/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A2nERgYezw1A",
      "metadata": {
        "id": "A2nERgYezw1A"
      },
      "source": [
        "# DeepTICI\n",
        "---\n",
        "This repository  is a fork of <https://github.com/IPMI-ICNS-UKE/DeepTICI>.\n",
        "Thanks to STROKE [publication](https://doi.org/10.1161/STROKEAHA.120.033807) for sharing the official code and model weights.\n",
        "\n",
        "The following code follows the license rights and limitations of the original shared code (CC BY-NC 4.0). You can view the full license text here:\n",
        "<https://raw.githubusercontent.com/IPMI-ICNS-UKE/DeepTICI/main/LICENSE.txt>\n",
        "\n",
        "\n",
        "**This code is provided for research and educational purposes only. Please do not use it for commercial purposes.**\n",
        "\n",
        "If you have any questions, please contact me at: xuchu_liu@rush.edu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bY_KC1wK89Eg",
      "metadata": {
        "id": "bY_KC1wK89Eg"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3euizQ2r9BUX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3euizQ2r9BUX",
        "outputId": "69965aa0-1392-452f-e715-f2a107666ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Collecting pydicom\n",
            "  Downloading pydicom-2.4.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install efficientnet_pytorch\n",
        "!pip install SimpleITK\n",
        "!pip install pydicom"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BPOj6eTT9VjY",
      "metadata": {
        "id": "BPOj6eTT9VjY"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper"
      ],
      "metadata": {
        "id": "h99Q81kdy5HT"
      },
      "id": "h99Q81kdy5HT"
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum, auto\n",
        "\n",
        "\"\"\"implements output and model modes to prevent using strings\"\"\"\n",
        "\n",
        "\n",
        "class OutputMode(Enum):\n",
        "    last_frame = auto()\n",
        "    all_frames = auto()\n",
        "\n",
        "\n",
        "class ModelMode(Enum):\n",
        "    train = auto()\n",
        "    inference = auto()\n"
      ],
      "metadata": {
        "id": "l3XJbH96zG_r"
      },
      "id": "l3XJbH96zG_r",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoader"
      ],
      "metadata": {
        "id": "9jIrKWq-zQRC"
      },
      "id": "9jIrKWq-zQRC"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from abc import ABC\n",
        "from typing import List, Tuple, Union, Literal\n",
        "\n",
        "import SimpleITK as sitk\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from skimage.filters import threshold_multiotsu\n",
        "\n",
        "\n",
        "class BaseDataProcessor(ABC):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"initializes a chain of preprocessor steps which is then executed for each elem\"\"\"\n",
        "        self._chain = []\n",
        "\n",
        "    def _chain_exe(self, image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"applies the preprocessor steps to an image, based on the order in the chain\n",
        "            arg: image: nd array\n",
        "            return: image as nd array\"\"\"\n",
        "        for processor_step, kwargs in self._chain:\n",
        "            if kwargs:\n",
        "                image = processor_step(image, **kwargs)\n",
        "            else:\n",
        "                image = processor_step(image)\n",
        "        return image.astype(np.float32)\n",
        "\n",
        "    def add_processor_step(self, func_name: str, kwargs: dict):\n",
        "        \"\"\"appends preprocessor step from name and kwargs to the chain\n",
        "        args:\n",
        "            func_name: name of the preprocessor function\n",
        "            kwargs: keyword arguments applied to the func_name together with nd image/array\"\"\"\n",
        "        func = getattr(self, func_name)\n",
        "        self._chain.append((func, kwargs))\n",
        "\n",
        "    @classmethod\n",
        "    def to_chain(cls, chain_elems: dict):\n",
        "        \"\"\"adds preprocesser functions to the chain with corresponding elements.\n",
        "        chain_elems contains the functain name as a key and kwargs as values\n",
        "        args:\n",
        "            chain_elems: is a dictionary with the preprocessor function name as key and the coressponding kwargs as\n",
        "            items. Items are a dict itself.\"\"\"\n",
        "        inst = cls()\n",
        "        for chain_elem, elem_kwargs in chain_elems.items():\n",
        "            inst.add_processor_step(chain_elem, elem_kwargs)\n",
        "        return inst\n",
        "\n",
        "    def __call__(self, image: np.ndarray, **kwargs):\n",
        "        \"\"\"executes chain\"\"\"\n",
        "        return self._chain_exe(image)\n",
        "\n",
        "\n",
        "class BasePreProcessor(BaseDataProcessor):\n",
        "    \"\"\"Implementation of Abstract Class BaseDataProcessor. Basic steps are implemented for preprocessing.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    \"\"\"all functions take an nd.array as input and return a n.array\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def temp_normalize_image(image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"normalizes images by substracting the mean along temporal axis\"\"\"\n",
        "        return image - np.mean(image, axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_image_range(image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"scales image values to an intervall of -1 to 1\"\"\"\n",
        "        return 2 * (image - image.min()) / (image.max() - image.min()) - 1\n",
        "\n",
        "    @staticmethod\n",
        "    def median_normalize_slice(image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"sets the image median to a value of 0\"\"\"\n",
        "        return image - np.median(image, axis=(-2, -1), keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def set_background_to_one(image: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"sets background pixel value to one\"\"\"\n",
        "        if image.sum() / np.prod(image.shape) < 0:\n",
        "            image = image * -1\n",
        "        return image\n",
        "\n",
        "\n",
        "class PreProcessor(BasePreProcessor):\n",
        "    \"\"\"implements more specicfic PreProcessor steps, that take additional arguments\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def clip_image(image: np.ndarray, boundaries: Tuple[float, float] = None,\n",
        "                   mode: Literal['multiotsu', 'median'] = None) -> np.ndarray:\n",
        "        \"\"\"clips image values to an intervall based on image intensieties.\n",
        "        Availabe are multiotsu thresholding, and above median\n",
        "        args:\n",
        "            boundaries: image is clipped by those fixed values, must be in the format(min,max). If mode is given,\n",
        "                        boundaries are ignored.\n",
        "            mode: must be either mutlitotsu or median. If multiotsu boundraries are based on two thresholds. If median\n",
        "                  the upper boundary is the median value and lower the min value of the image \"\"\"\n",
        "\n",
        "        if mode == 'multiotsu':\n",
        "            boundaries = threshold_multiotsu(image, classes=3)\n",
        "        elif mode == 'median':\n",
        "            boundaries = [image.min(), None]\n",
        "            boundaries[1] = np.median(image)\n",
        "        elif not boundaries and not mode:\n",
        "            boundaries = [image.min(), image.max()]\n",
        "        return np.clip(image, boundaries[0], boundaries[-1])\n",
        "\n",
        "    @staticmethod\n",
        "    def resample_image(image: np.ndarray, frame_shape=(380, 380)) -> np.ndarray:\n",
        "        \"\"\"resamples image to specified pixel size using nearest neighbours\n",
        "        args:\n",
        "            frame_shape: crops the last two dimensions to the given value\"\"\"\n",
        "        num_frames = len(image)\n",
        "        resized_image = np.zeros((num_frames,) + frame_shape)\n",
        "        for idx, frame in enumerate(image):\n",
        "            resized_image[idx] = cv2.resize(frame, frame_shape, interpolation=cv2.INTER_NEAREST)\n",
        "        return resized_image\n",
        "\n",
        "    @staticmethod\n",
        "    def crop_image(image: np.ndarray, tol=0.01) -> np.ndarray:\n",
        "        \"\"\"removes bars (with noise) on image edges. tol is the amount of noise allowed.\n",
        "            args:\n",
        "                tol: tolerance value above which axis are accepted to contain image information. If a whole row\n",
        "                     contains no information and is at the image edge it is assumed to contain no value for\n",
        "                     classifiation\"\"\"\n",
        "        mask = np.std(image, axis=0) > tol\n",
        "        idx = np.ix_(mask.any(1), mask.any(0))\n",
        "        return image[:, idx[0], idx[1]]\n",
        "\n",
        "    @staticmethod\n",
        "    def center_crop(image, rel_size=(0.5, 0.5)) -> np.ndarray:\n",
        "        \"\"\"takes the center crop of a given image. rel_size is the relative size of the center crop in realtion to the\n",
        "         input image\n",
        "         args:\n",
        "            rel_size: relative size of the center crop in respect to the orginal image. \"\"\"\n",
        "        x_size, y_size = image.shape[-2:]\n",
        "        pos_x_1 = int(x_size * (1 - rel_size[0]) / 2)\n",
        "        pos_y_1 = int(y_size * (1 - rel_size[1]) / 2)\n",
        "        pos_x_2 = int(pos_x_1 + x_size * rel_size[0])\n",
        "        pos_y_2 = int(pos_y_1 + y_size * rel_size[1])\n",
        "        image = image[..., pos_x_1:pos_x_2, pos_y_1:pos_y_2]\n",
        "        return image\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, preprocessor_steps: dict):\n",
        "        \"\"\"class to load dicom images from path, gets preprocessor arguments to build the preprocessor\n",
        "        args:\n",
        "            preprocessor_steps: dict containing names and kwargs of the preprocessing steps\"\"\"\n",
        "        self.preprocessor = PreProcessor.to_chain(preprocessor_steps)\n",
        "\n",
        "    def __call__(self, series_paths: List[Union[str, os.PathLike]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"loads images to the RAM and applies the preprocessor steps\n",
        "        args:\n",
        "            series_paths: List of paths. First entry must point to AP and second to the lateral view. Views must be the\n",
        "                          same the length.\n",
        "            return: A tuple containing, two tensors 1.) both views shape: 1 (dummy batch) x time x 2 (views) x height x\n",
        "                    width 2.) length of the series \"\"\"\n",
        "\n",
        "        # loads both dicom views\n",
        "        series = [self._load_image(path) for path in series_paths]\n",
        "        series_lengths = [len(view) for view in series]\n",
        "        assert len(series_lengths) == 2, f'expected 2 views, got {len(series_lengths)} views'\n",
        "        assert series_lengths[0] == series_lengths[1], f'Length of DSA views are not equal'\n",
        "\n",
        "        # converts views to array and make a 3Ch image. The 3Ch are filled with consecutive frames\n",
        "        series = np.array(series)\n",
        "        series = self._make_3_ch_img(series)\n",
        "        series = torch.tensor(series, dtype=torch.float32).unsqueeze(dim=0)\n",
        "        return series, torch.tensor(series_lengths[0]).unsqueeze(dim=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_3_ch_img(img: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"creates a 3Ch image from a 1Ch image. The 3Ch consists of 3 consecutive frames, centered around the original\n",
        "        Frame\n",
        "        arg:\n",
        "            img: 1CH Image\n",
        "        return: same image with 3CH. Each slice from the orginal image is converted to the center channel sourounded by\n",
        "                the previous and nect slice.\"\"\"\n",
        "        new_img = np.zeros((*img.shape[0:2], 3, *img.shape[-2:])).swapaxes(1, 0)\n",
        "        for i_frame in range(len(new_img)):\n",
        "            img_temp = np.zeros(new_img.shape[1:])\n",
        "            for i_channel in range(img_temp.shape[1]):\n",
        "                if i_frame == 0 and i_channel == 0:\n",
        "                    pass\n",
        "                elif i_frame == len(new_img) - 1 and i_channel == 2:\n",
        "                    pass\n",
        "                else:\n",
        "                    img_temp[:, i_channel] = img[:, i_frame - 1 + i_channel]\n",
        "            new_img[i_frame] = img_temp\n",
        "        return new_img\n",
        "\n",
        "    def _load_image(self, image_path: Union[str, os.PathLike]) -> np.ndarray:\n",
        "        \"\"\"loads dicom images to RAM and preprocesses image from path\n",
        "            args:\n",
        "                image_paths: list containing path to two views order: [ap, lat]\n",
        "            return: preprocessed dicom image as np.array\"\"\"\n",
        "        image = self._load_img_to_ram(image_path)\n",
        "        image = self.preprocessor(image)\n",
        "        return image.astype(np.half)\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_img_to_ram(file_path: Union[str, os.PathLike]) -> np.ndarray:\n",
        "        \"\"\"loads dicom image to RAM\n",
        "            args:\n",
        "                image_path: path pointing to an dicom image\n",
        "            return: dicom image as np.array\"\"\"\n",
        "        assert file_path.endswith('.dcm'), 'only Dicom file format is supported'\n",
        "        image = sitk.GetArrayFromImage(sitk.ReadImage(file_path))\n",
        "        return image\n"
      ],
      "metadata": {
        "id": "JH6GUqGRzoO5"
      },
      "id": "JH6GUqGRzoO5",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "KsBNsPzk09PT"
      },
      "id": "KsBNsPzk09PT"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "from typing import Union, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "class SwishImplementation(torch.autograd.Function):\n",
        "    \"\"\"legacy method used before torch.nn.SiLU was available\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, i):\n",
        "        result = i * torch.sigmoid(i)\n",
        "        ctx.save_for_backward(i)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        i = ctx.saved_tensors[0]\n",
        "        sigmoid_i = torch.sigmoid(i)\n",
        "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"pytorch implementation of Swish activation function\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return SwishImplementation.apply(x)\n",
        "\n",
        "\n",
        "class EfficientTwoArmEncoder(nn.Module):\n",
        "    \"\"\"Implements a two-arm-encoder network based on an efficient-net b0 backbone. This networks fuses each frame of two\n",
        "     views to one latent representation by combining them before the last convolutional layer.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 7, in_channels: int = 3, feature_size: int = 1280):\n",
        "        \"\"\"args:\n",
        "            num_classes: Number of classes if this network is used for classification.\n",
        "            in_channels: Number of channels in the input image\n",
        "            feature_size: size of the latent space\"\"\"\n",
        "        super().__init__()\n",
        "        # Efficient-net backbone\n",
        "        self.feature_extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,\n",
        "                                                              in_channels=in_channels)\n",
        "        self.swish = Swish()\n",
        "\n",
        "        # combines latent representation of the two views\n",
        "        self.combine_layer = nn.Sequential(\n",
        "            nn.Conv2d(320 * 2, feature_size, 1, bias=False),\n",
        "            nn.BatchNorm2d(feature_size, momentum=0.01, eps=1e-3),\n",
        "            self.swish\n",
        "        )\n",
        "\n",
        "        # classification layer\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.Linear(feature_size, num_classes),\n",
        "            Swish(),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
        "        \"\"\"args:\n",
        "            x: Tensor of size batch_size x 2 (num_views) x frames x channels x height x width\n",
        "            returns: latent representation of x batch_size x frames x feature_size\"\"\"\n",
        "\n",
        "        x_shape = x.shape\n",
        "        # flatten image to batch_size*views*frames x channels x height x width\n",
        "        x = x.contiguous().view((-1, *x_shape[-3:]))\n",
        "\n",
        "        # encode all frames and views simultaneously\n",
        "        x = self.extract_features(x)\n",
        "\n",
        "        # unflatten to dimension: batch_size*frames x channels*views x height x width\n",
        "        x = x.view((-1, x.shape[-3] * 2, *x.shape[-2:]))\n",
        "        x = self.combine_layer(x)\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = x.squeeze(dim=2).squeeze(dim=2)\n",
        "\n",
        "        # reshape to batch_size x frames x feature_size\n",
        "        x = x.view((*x_shape[:2], x.shape[-1]))\n",
        "        return x\n",
        "\n",
        "    def extract_features(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"applies all layers of efficient-net b0 until the last convolution\n",
        "        args:\n",
        "            x: input tensor shape: n x channels x height x width\"\"\"\n",
        "        x = self.swish(self.feature_extractor._bn0(self.feature_extractor._conv_stem(x)))\n",
        "        for idx, block in enumerate(self.feature_extractor._blocks):\n",
        "            drop_connect_rate = self.feature_extractor._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self.feature_extractor._blocks)\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FastGRU(nn.Module):\n",
        "    def __init__(self, feature_size: int, output_size: int = None):\n",
        "        \"\"\"intializes weights of the GRU with three gates: Input, forget and hidden.\n",
        "        args:\n",
        "            feature_size: Size of the input in the last dimension\n",
        "            output_size: size of the cell state/ output. if not given it is equal to feature_size\"\"\"\n",
        "        super().__init__()\n",
        "        self.input_size = feature_size\n",
        "        if not output_size:\n",
        "            self.hidden_size = self.input_size\n",
        "        else:\n",
        "            self.hidden_size = output_size\n",
        "        self.W = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size * 3))\n",
        "        self.U = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size * 3))\n",
        "        self.bias = nn.Parameter(torch.Tensor(self.hidden_size * 3))\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"implements GRU forward pass with three gates. Input, forget and output. The input gate controls the relevance\n",
        "         of new timesteps, the forget gate the relevance of the cell state in regard to the new input and the output\n",
        "         gate controls the new cellstate.\n",
        "         args:\n",
        "            x: input tensor of shape batch x timesteps x feature_size\"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        hidden_seq = []\n",
        "        dropout_prob = 0.1\n",
        "        h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "                    torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "\n",
        "        x = F.dropout(input=x, p=dropout_prob, training=self.training)\n",
        "\n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            gates = x_t @ self.U + c_t @ self.W + self.bias\n",
        "\n",
        "            z_t, r_t = (\n",
        "                # input\n",
        "                torch.sigmoid(gates[:, :self.hidden_size]),\n",
        "                # forget\n",
        "                torch.sigmoid(gates[:, self.hidden_size:self.hidden_size * 2])\n",
        "            )\n",
        "\n",
        "            x_t = torch.tanh(r_t * x_t @ self.U + c_t @ self.W + self.bias)[:,\n",
        "                  self.hidden_size * 2:self.hidden_size * 3]\n",
        "\n",
        "            c_t = (1 - z_t) * c_t + x_t\n",
        "\n",
        "            hidden_seq.append(c_t.unsqueeze(0))\n",
        "\n",
        "            c_t = F.dropout(input=c_t, p=dropout_prob, training=self.training)\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "\n",
        "        return hidden_seq\n",
        "\n",
        "\n",
        "class TICIModelHandler(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes: int,\n",
        "                 feature_size: int,\n",
        "                 pretrained: Union[None, Union[str, os.PathLike], List[Union[str, os.PathLike]]] = None,\n",
        "                 in_channels: int = 3,\n",
        "                 output_size: int = None\n",
        "                 ):\n",
        "        \"\"\"Wrapper around Encoder+GRU+Classifier structure. Serves model loading and selecting the right timesteps for\n",
        "        training and inference.\n",
        "        args:\n",
        "            num_classes: number of classes in the output\n",
        "            feature_size: output size of the encoder network\n",
        "            pretrained: path(s) to pickeld weights, if multiple paths are given an ensamble is applied\n",
        "            in_channels: expected number of channels in the input image\n",
        "            output_size: size of the cell state for the GRU. If not given equal to feature_size\"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.network = TICITemporalNetwork(in_channels=in_channels, feature_size=feature_size, output_size=output_size,\n",
        "                                           num_classes=num_classes)\n",
        "        self.pretrained = False\n",
        "        if pretrained:\n",
        "            if isinstance(pretrained, list):\n",
        "                self.pretrained = pretrained\n",
        "            else:\n",
        "                # if multiple paths ensemble over all given weights\n",
        "                self.pretrained = True\n",
        "                self.load_model(pretrained)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        \"\"\"load state dict of the network.\n",
        "        args:\n",
        "            path: path to weights\"\"\"\n",
        "        state_dict = torch.load(path)\n",
        "        while any(['module.' in k for k in state_dict.keys()]):\n",
        "            # remove eventual pytorch wrapper\n",
        "            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "        self.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    @autocast()\n",
        "    def forward(self, x: torch.Tensor, series_length: torch.Tensor = None, model_mode: ModelMode = ModelMode.inference,\n",
        "                output_mode: OutputMode = OutputMode.last_frame) -> torch.tensor:\n",
        "        if x.dim() == 5:\n",
        "            x = x.unsqueeze(dim=0)\n",
        "        assert x.dim() == 6, 'Input must be 4 or 5 dimensional ((batch) x time x view x channels x height x width'\n",
        "        if series_length:\n",
        "            if series_length.dim() == 0:\n",
        "                series_length = series_length.unsqueeze(dim=0)\n",
        "\n",
        "        series_length = series_length.to(x.device)\n",
        "        assert isinstance(model_mode, ModelMode), 'forward pass mode is not implemented (yet)'\n",
        "\n",
        "        # inference\n",
        "        if model_mode == ModelMode.inference:\n",
        "            with torch.no_grad():\n",
        "                # ensemble\n",
        "                if isinstance(self.pretrained, list):\n",
        "                    for path in self.pretrained:\n",
        "                        self.load_model(path)\n",
        "                        self.eval()\n",
        "                        try:\n",
        "                            predictions += self.network(x)\n",
        "                        except NameError:\n",
        "                            predictions = self.network(x)\n",
        "                    predictions /= len(self.pretrained)\n",
        "                else:\n",
        "                    self.eval()\n",
        "                    predictions = self.network(x)\n",
        "        # training\n",
        "        elif model_mode == ModelMode.train:\n",
        "            self.train()\n",
        "            predictions = self.network(x)\n",
        "        else:\n",
        "            raise NotImplementedError('forward pass mode is not implemented (yet)')\n",
        "\n",
        "        assert isinstance(output_mode, OutputMode), 'output mode is not implemented (yet)'\n",
        "        if output_mode == OutputMode.last_frame:\n",
        "            predictions = self._get_last_frame_in_batch(predictions, series_length)\n",
        "        elif output_mode == OutputMode.all_frames:\n",
        "            pass\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_last_frame_in_batch(predictions: torch.Tensor, series_length: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"iterates through batch to fetch the last frame of each input\n",
        "        args:\n",
        "            predictions: tensor of shape batch x time x num_classes\n",
        "            series_length: tensor containing the series lengths of batch elements. If not specified the last frame is\n",
        "            considered\"\"\"\n",
        "        if not series_length:\n",
        "            pred = predictions[:, -1]\n",
        "        else:\n",
        "            pred = torch.zeros((predictions.shape[0], predictions.shape[-1]), device=predictions.device)\n",
        "            for i, _series_length in enumerate(series_length):\n",
        "                pred[i] = predictions[i, _series_length - 1]\n",
        "        return pred\n",
        "\n",
        "\n",
        "class TICITemporalNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels: int,\n",
        "                 output_size: int,\n",
        "                 num_classes: int,\n",
        "                 feature_size: int) -> torch.Tensor:\n",
        "        \"\"\"Wraps around encoder, GRU and classifier.\n",
        "        args:\n",
        "            in_channels: number of channels in input\n",
        "            output_size: size of the GRUs cell state\n",
        "            num_classes: number of classes in the dataset\n",
        "            feature_size: number of features from the encoder\"\"\"\n",
        "        super().__init__()\n",
        "        self.encoder = EfficientTwoArmEncoder(feature_size=feature_size, in_channels=in_channels,\n",
        "                                              num_classes=num_classes)\n",
        "        self.gru = FastGRU(feature_size=feature_size, output_size=output_size)\n",
        "        self.classifier = nn.Sequential(nn.Dropout(),\n",
        "                                        nn.Linear(feature_size, num_classes),\n",
        "                                        Swish(),\n",
        "                                        nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"forward pass through all model parts:\n",
        "        args:\n",
        "            x: input tensor of shape batch x time x views x channels x height x width\"\"\"\n",
        "        x = self.encoder(x, mode='encoder')\n",
        "        x = self.gru(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "-w-p_T570irE"
      },
      "id": "-w-p_T570irE",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "A9YxtrrO1KVF"
      },
      "id": "A9YxtrrO1KVF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load DSA series pipeline"
      ],
      "metadata": {
        "id": "5elwcbCV3gKJ"
      },
      "id": "5elwcbCV3gKJ"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor_steps = {\n",
        "    'crop_image': {'tol': 0.01},\n",
        "    'clip_image': {'mode': 'multiotsu'},\n",
        "    'temp_normalize_image': None,\n",
        "    'normalize_image_range': None,\n",
        "    'resample_image': {\n",
        "        'frame_shape': (224, 224)\n",
        "    }\n",
        "}\n",
        "data_loader = DataLoader(preprocessor_steps)"
      ],
      "metadata": {
        "id": "CrJ9N4Gp4n1p"
      },
      "id": "CrJ9N4Gp4n1p",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model weights"
      ],
      "metadata": {
        "id": "A1HkOpDl4oDZ"
      },
      "id": "A1HkOpDl4oDZ"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/IPMI-ICNS-UKE/DeepTICI/raw/main/model_weights/experiment_I"
      ],
      "metadata": {
        "id": "VCgJcMxr0iwT",
        "outputId": "d2770fd4-11fc-4b5d-b991-91e7c5f319c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VCgJcMxr0iwT",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-13 21:20:29--  https://github.com/IPMI-ICNS-UKE/DeepTICI/raw/main/model_weights/experiment_I\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/IPMI-ICNS-UKE/DeepTICI/main/model_weights/experiment_I [following]\n",
            "--2023-12-13 21:20:29--  https://raw.githubusercontent.com/IPMI-ICNS-UKE/DeepTICI/main/model_weights/experiment_I\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59091797 (56M) [application/octet-stream]\n",
            "Saving to: ‘experiment_I’\n",
            "\n",
            "experiment_I        100%[===================>]  56.35M   166MB/s    in 0.3s    \n",
            "\n",
            "2023-12-13 21:20:32 (166 MB/s) - ‘experiment_I’ saved [59091797/59091797]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_params = {\n",
        "    'pretrained': 'experiment_I',\n",
        "    'feature_size': 1280,\n",
        "    'num_classes': 6,\n",
        "    'in_channels': 3,\n",
        "    'output_size': 1280\n",
        "}\n",
        "model = TICIModelHandler(**model_params)"
      ],
      "metadata": {
        "id": "Xt_SlMPT0iz3",
        "outputId": "80ea97a8-7881-41b0-d2dc-5fcb472b36ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Xt_SlMPT0iz3",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n",
            "100%|██████████| 20.4M/20.4M [00:00<00:00, 124MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict"
      ],
      "metadata": {
        "id": "L2FtGExY05BR"
      },
      "id": "L2FtGExY05BR"
    },
    {
      "cell_type": "markdown",
      "id": "dqPv6Jms-Udn",
      "metadata": {
        "id": "dqPv6Jms-Udn"
      },
      "source": [
        "### Please upload your files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![upload_guide](https://github.com/Achillesy/Fetal_Functional_MRI_Segmentation/blob/master/figures/upload_guide.png?raw=1)\n",
        "1. Click the **Files** icon on the left\n",
        "2. Click the **Upload to session storage** icon above\n",
        "3. Your uploaded files will be displayed here\n",
        "----\n",
        "After double-checking your uploaded files, by <font color=\"green\">pressing the **Enter** key in the input box below</font>, the fMRI mask will be automatically generated in a short time."
      ],
      "metadata": {
        "id": "GD0eE3A57Jff"
      },
      "id": "GD0eE3A57Jff"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "26b1a6e0-4b17-40a4-a45c-632546836c0a",
      "metadata": {
        "id": "26b1a6e0-4b17-40a4-a45c-632546836c0a",
        "outputId": "63b8982b-4e4c-43b2-a0a2-2bc9b6f86f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "input()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import pydicom\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RFxygYdW5f9E"
      },
      "id": "RFxygYdW5f9E",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dcm_path = glob(\"*.dcm\")\n",
        "assert(len(dcm_path) == 2)\n",
        "dcm_0 = pydicom.read_file(dcm_path[0])\n",
        "img_0 = dcm_0.pixel_array\n",
        "dcm_1 = pydicom.read_file(dcm_path[1])\n",
        "img_1 = dcm_1.pixel_array\n",
        "assert(dcm_0.PatientID == dcm_1.PatientID and dcm_0.PatientName == dcm_1.PatientName)\n",
        "assert(img_0.shape == img_1.shape)\n",
        "print(dcm_0.PatientID, dcm_0.PatientName, img_0.shape)\n"
      ],
      "metadata": {
        "id": "ELs7LSRzEFvR",
        "outputId": "4edf7885-b1b0-42f8-e402-c86bf94a23ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ELs7LSRzEFvR",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8740638 CAPLIN^PHOEBE SELENE^^^ (21, 1024, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0a99e9bc-f612-47ab-a113-184b024170b1",
      "metadata": {
        "id": "0a99e9bc-f612-47ab-a113-184b024170b1",
        "outputId": "c152458f-4a7c-4e59-f2ce-7dac2133787d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "label_mapping = {\n",
        "    0: 'TICI 0',\n",
        "    1: 'TICI 0',\n",
        "    2: 'TICI 2a',\n",
        "    3: 'TICI 2b',\n",
        "    4: 'TICI 3',\n",
        "    5: 'TICI 3'\n",
        "}\n",
        "\n",
        "data = data_loader(dcm_path)\n",
        "raw_values = model(*data, model_mode=ModelMode.inference, output_mode=OutputMode.last_frame)\n",
        "values_array = raw_values.numpy()\n",
        "score = np.argmax(values_array)\n",
        "\n",
        "plt.bar(range(len(values_array[0])), values_array[0])\n",
        "plt.text(score, values_array[0, score], f'Max: {values_array[0, score]:.4f}', ha='center', va='bottom')\n",
        "plt.xlabel('label_mapping')\n",
        "plt.ylabel('Values')\n",
        "plt.title(f\"Predict: '{label_mapping[score]}'\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L0MKNDE0-e2o",
      "metadata": {
        "id": "L0MKNDE0-e2o"
      },
      "source": [
        "## Clean up temporary files\n",
        "\n",
        "Warning\n",
        "Ensure that your files are saved elsewhere. This runtime's files will be deleted when this runtime is terminated. More info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d4a32d1-b747-47d4-81a5-f834fb1c94c7",
      "metadata": {
        "id": "6d4a32d1-b747-47d4-81a5-f834fb1c94c7"
      },
      "outputs": [],
      "source": [
        "!rm *.dcm"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}