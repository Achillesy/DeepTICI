{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc0b844",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Achillesy/DeepTICI/blob/main/predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A2nERgYezw1A",
   "metadata": {
    "id": "A2nERgYezw1A"
   },
   "source": [
    "# DeepTICI\n",
    "---\n",
    "This repository  is a fork of <https://github.com/IPMI-ICNS-UKE/DeepTICI>.\n",
    "Thanks to STROKE [publication](https://doi.org/10.1161/STROKEAHA.120.033807) for sharing the official code and model weights.\n",
    "\n",
    "The following code follows the license rights and limitations of the original shared code (CC BY-NC 4.0). You can view the full license text here:\n",
    "<https://raw.githubusercontent.com/IPMI-ICNS-UKE/DeepTICI/main/LICENSE.txt>\n",
    "\n",
    "\n",
    "**This code is provided for research and educational purposes only. Please do not use it for commercial purposes.**\n",
    "\n",
    "If you have any questions, please contact me at: xuchu_liu@rush.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bY_KC1wK89Eg",
   "metadata": {
    "id": "bY_KC1wK89Eg"
   },
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3euizQ2r9BUX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3euizQ2r9BUX",
    "outputId": "66e14c76-9bc0-4d34-f22b-1f94a3b7200b"
   },
   "outputs": [],
   "source": [
    "# !pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BPOj6eTT9VjY",
   "metadata": {
    "id": "BPOj6eTT9VjY"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hijfpsYf9bcn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hijfpsYf9bcn",
    "outputId": "a504af02-b2df-4be0-b1d2-75bc2968ebbc"
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/IPMI-ICNS-UKE/DeepTICI/raw/main/model_weights/experiment_I\n",
    "# !wget https://github.com/IPMI-ICNS-UKE/DeepTICI/raw/main/model_weights/experiment_III_0\n",
    "# !wget https://github.com/IPMI-ICNS-UKE/DeepTICI/raw/main/model_weights/experiment_III_1\n",
    "# !wget https://github.com/IPMI-ICNS-UKE/DeepTICI/raw/main/model_weights/experiment_III_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MsOkHPpD-Q_J",
   "metadata": {
    "id": "MsOkHPpD-Q_J"
   },
   "source": [
    "## Load model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02884ef-7e31-45ca-ae98-4a18d5b9379c",
   "metadata": {
    "id": "d02884ef-7e31-45ca-ae98-4a18d5b9379c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "# import yaml\n",
    "import numpy as np\n",
    "from typing import Union, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dqPv6Jms-Udn",
   "metadata": {
    "id": "dqPv6Jms-Udn"
   },
   "source": [
    "## Please upload your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1a6e0-4b17-40a4-a45c-632546836c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "487a507e-9449-411e-9174-371ca1e7f095",
   "metadata": {},
   "source": [
    "## helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d405179-458a-4ac2-b32c-3f7da0919b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f7f49d-340c-4f62-be90-87fbed5cdf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputMode(Enum):\n",
    "    last_frame = auto()\n",
    "    all_frames = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ba3079-7dcd-4951-94c7-9c83fac57295",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMode(Enum):\n",
    "    train = auto()\n",
    "    inference = auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77a09d9-32f6-4a9a-8ade-b53668711cd7",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae77618-b1be-4688-867d-5b17493c1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwishImplementation(torch.autograd.Function):\n",
    "    \"\"\"legacy method used before torch.nn.SiLU was available\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.sigmoid(i)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_tensors[0]\n",
    "        sigmoid_i = torch.sigmoid(i)\n",
    "        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55ed7c1f-02fe-45cc-aa10-366c42e6a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    \"\"\"pytorch implementation of Swish activation function\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return SwishImplementation.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd323847-3e5b-4597-aa7d-644077bd5f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientTwoArmEncoder(nn.Module):\n",
    "    \"\"\"Implements a two-arm-encoder network based on an efficient-net b0 backbone. This networks fuses each frame of two\n",
    "     views to one latent representation by combining them before the last convolutional layer.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 7, in_channels: int = 3, feature_size: int = 1280):\n",
    "        \"\"\"args:\n",
    "            num_classes: Number of classes if this network is used for classification.\n",
    "            in_channels: Number of channels in the input image\n",
    "            feature_size: size of the latent space\"\"\"\n",
    "        super().__init__()\n",
    "        # Efficient-net backbone\n",
    "        self.feature_extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,\n",
    "                                                              in_channels=in_channels)\n",
    "        self.swish = Swish()\n",
    "\n",
    "        # combines latent representation of the two views\n",
    "        self.combine_layer = nn.Sequential(\n",
    "            nn.Conv2d(320 * 2, feature_size, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_size, momentum=0.01, eps=1e-3),\n",
    "            self.swish\n",
    "        )\n",
    "\n",
    "        # classification layer\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(feature_size, num_classes),\n",
    "            Swish(),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"args:\n",
    "            x: Tensor of size batch_size x 2 (num_views) x frames x channels x height x width\n",
    "            returns: latent representation of x batch_size x frames x feature_size\"\"\"\n",
    "\n",
    "        x_shape = x.shape\n",
    "        # flatten image to batch_size*views*frames x channels x height x width\n",
    "        x = x.contiguous().view((-1, *x_shape[-3:]))\n",
    "\n",
    "        # encode all frames and views simultaneously\n",
    "        x = self.extract_features(x)\n",
    "\n",
    "        # unflatten to dimension: batch_size*frames x channels*views x height x width\n",
    "        x = x.view((-1, x.shape[-3] * 2, *x.shape[-2:]))\n",
    "        x = self.combine_layer(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = x.squeeze(dim=2).squeeze(dim=2)\n",
    "\n",
    "        # reshape to batch_size x frames x feature_size\n",
    "        x = x.view((*x_shape[:2], x.shape[-1]))\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"applies all layers of efficient-net b0 until the last convolution\n",
    "        args:\n",
    "            x: input tensor shape: n x channels x height x width\"\"\"\n",
    "        x = self.swish(self.feature_extractor._bn0(self.feature_extractor._conv_stem(x)))\n",
    "        for idx, block in enumerate(self.feature_extractor._blocks):\n",
    "            drop_connect_rate = self.feature_extractor._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self.feature_extractor._blocks)\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989aee4c-4548-40de-b803-4a2c18b1b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastGRU(nn.Module):\n",
    "    def __init__(self, feature_size: int, output_size: int = None):\n",
    "        \"\"\"intializes weights of the GRU with three gates: Input, forget and hidden.\n",
    "        args:\n",
    "            feature_size: Size of the input in the last dimension\n",
    "            output_size: size of the cell state/ output. if not given it is equal to feature_size\"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = feature_size\n",
    "        if not output_size:\n",
    "            self.hidden_size = self.input_size\n",
    "        else:\n",
    "            self.hidden_size = output_size\n",
    "        self.W = nn.Parameter(torch.Tensor(self.input_size, self.hidden_size * 3))\n",
    "        self.U = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size * 3))\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.hidden_size * 3))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"implements GRU forward pass with three gates. Input, forget and output. The input gate controls the relevance\n",
    "         of new timesteps, the forget gate the relevance of the cell state in regard to the new input and the output\n",
    "         gate controls the new cellstate.\n",
    "         args:\n",
    "            x: input tensor of shape batch x timesteps x feature_size\"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        dropout_prob = 0.1\n",
    "        h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "                    torch.zeros(bs, self.hidden_size).to(x.device))\n",
    "\n",
    "        x = F.dropout(input=x, p=dropout_prob, training=self.training)\n",
    "\n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "\n",
    "            gates = x_t @ self.U + c_t @ self.W + self.bias\n",
    "\n",
    "            z_t, r_t = (\n",
    "                # input\n",
    "                torch.sigmoid(gates[:, :self.hidden_size]),\n",
    "                # forget\n",
    "                torch.sigmoid(gates[:, self.hidden_size:self.hidden_size * 2])\n",
    "            )\n",
    "\n",
    "            x_t = torch.tanh(r_t * x_t @ self.U + c_t @ self.W + self.bias)[:,\n",
    "                  self.hidden_size * 2:self.hidden_size * 3]\n",
    "\n",
    "            c_t = (1 - z_t) * c_t + x_t\n",
    "\n",
    "            hidden_seq.append(c_t.unsqueeze(0))\n",
    "\n",
    "            c_t = F.dropout(input=c_t, p=dropout_prob, training=self.training)\n",
    "\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "\n",
    "        return hidden_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a5e5406-09b0-4ce2-826b-9014de7284bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TICIModelHandler(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes: int,\n",
    "                 feature_size: int,\n",
    "                 pretrained: Union[None, Union[str, os.PathLike], List[Union[str, os.PathLike]]] = None,\n",
    "                 in_channels: int = 3,\n",
    "                 output_size: int = None\n",
    "                 ):\n",
    "        \"\"\"Wrapper around Encoder+GRU+Classifier structure. Serves model loading and selecting the right timesteps for\n",
    "        training and inference.\n",
    "        args:\n",
    "            num_classes: number of classes in the output\n",
    "            feature_size: output size of the encoder network\n",
    "            pretrained: path(s) to pickeld weights, if multiple paths are given an ensamble is applied\n",
    "            in_channels: expected number of channels in the input image\n",
    "            output_size: size of the cell state for the GRU. If not given equal to feature_size\"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.network = TICITemporalNetwork(in_channels=in_channels, feature_size=feature_size, output_size=output_size,\n",
    "                                           num_classes=num_classes)\n",
    "        self.pretrained = False\n",
    "        if pretrained:\n",
    "            if isinstance(pretrained, list):\n",
    "                self.pretrained = pretrained\n",
    "            else:\n",
    "                # if multiple paths ensemble over all given weights\n",
    "                self.pretrained = True\n",
    "                self.load_model(pretrained)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"load state dict of the network.\n",
    "        args:\n",
    "            path: path to weights\"\"\"\n",
    "        state_dict = torch.load(path)\n",
    "        while any(['module.' in k for k in state_dict.keys()]):\n",
    "            # remove eventual pytorch wrapper\n",
    "            state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "        self.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    @autocast()\n",
    "    def forward(self, x: torch.Tensor, series_length: torch.Tensor = None, model_mode: ModelMode = ModelMode.inference,\n",
    "                output_mode: OutputMode = OutputMode.last_frame) -> torch.tensor:\n",
    "        if x.dim() == 5:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        assert x.dim() == 6, 'Input must be 4 or 5 dimensional ((batch) x time x view x channels x height x width'\n",
    "        if series_length:\n",
    "            if series_length.dim() == 0:\n",
    "                series_length = series_length.unsqueeze(dim=0)\n",
    "\n",
    "        series_length = series_length.to(x.device)\n",
    "        assert isinstance(model_mode, ModelMode), 'forward pass mode is not implemented (yet)'\n",
    "\n",
    "        # inference\n",
    "        if model_mode == ModelMode.inference:\n",
    "            with torch.no_grad():\n",
    "                # ensemble\n",
    "                if isinstance(self.pretrained, list):\n",
    "                    for path in self.pretrained:\n",
    "                        self.load_model(path)\n",
    "                        self.eval()\n",
    "                        try:\n",
    "                            predictions += self.network(x)\n",
    "                        except NameError:\n",
    "                            predictions = self.network(x)\n",
    "                    predictions /= len(self.pretrained)\n",
    "                else:\n",
    "                    self.eval()\n",
    "                    predictions = self.network(x)\n",
    "        # training\n",
    "        elif model_mode == ModelMode.train:\n",
    "            self.train()\n",
    "            predictions = self.network(x)\n",
    "        else:\n",
    "            raise NotImplementedError('forward pass mode is not implemented (yet)')\n",
    "\n",
    "        assert isinstance(output_mode, OutputMode), 'output mode is not implemented (yet)'\n",
    "        if output_mode == OutputMode.last_frame:\n",
    "            predictions = self._get_last_frame_in_batch(predictions, series_length)\n",
    "        elif output_mode == OutputMode.all_frames:\n",
    "            pass\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_last_frame_in_batch(predictions: torch.Tensor, series_length: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"iterates through batch to fetch the last frame of each input\n",
    "        args:\n",
    "            predictions: tensor of shape batch x time x num_classes\n",
    "            series_length: tensor containing the series lengths of batch elements. If not specified the last frame is\n",
    "            considered\"\"\"\n",
    "        if not series_length:\n",
    "            pred = predictions[:, -1]\n",
    "        else:\n",
    "            pred = torch.zeros((predictions.shape[0], predictions.shape[-1]), device=predictions.device)\n",
    "            for i, _series_length in enumerate(series_length):\n",
    "                pred[i] = predictions[i, _series_length - 1]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc98ea78-43d0-4a81-bb4e-7d843ff08459",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TICITemporalNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 output_size: int,\n",
    "                 num_classes: int,\n",
    "                 feature_size: int) -> torch.Tensor:\n",
    "        \"\"\"Wraps around encoder, GRU and classifier.\n",
    "        args:\n",
    "            in_channels: number of channels in input\n",
    "            output_size: size of the GRUs cell state\n",
    "            num_classes: number of classes in the dataset\n",
    "            feature_size: number of features from the encoder\"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = EfficientTwoArmEncoder(feature_size=feature_size, in_channels=in_channels,\n",
    "                                              num_classes=num_classes)\n",
    "        self.gru = FastGRU(feature_size=feature_size, output_size=output_size)\n",
    "        self.classifier = nn.Sequential(nn.Dropout(),\n",
    "                                        nn.Linear(feature_size, num_classes),\n",
    "                                        Swish(),\n",
    "                                        nn.Softmax(dim=-1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"forward pass through all model parts:\n",
    "        args:\n",
    "            x: input tensor of shape batch x time x views x channels x height x width\"\"\"\n",
    "        x = self.encoder(x, mode='encoder')\n",
    "        x = self.gru(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d999d8-f4e1-4ada-9f59-28491ecf0ff9",
   "metadata": {},
   "source": [
    "## data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c33f9a29-2634-4ce6-9383-fada1a9ed553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "from abc import ABC\n",
    "from typing import List, Tuple, Union, Literal\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import cv2\n",
    "# import numpy as np\n",
    "# import torch\n",
    "from skimage.filters import threshold_multiotsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb6f6b1f-ba7d-4145-a88e-df683e55b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataProcessor(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"initializes a chain of preprocessor steps which is then executed for each elem\"\"\"\n",
    "        self._chain = []\n",
    "\n",
    "    def _chain_exe(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"applies the preprocessor steps to an image, based on the order in the chain\n",
    "            arg: image: nd array\n",
    "            return: image as nd array\"\"\"\n",
    "        for processor_step, kwargs in self._chain:\n",
    "            if kwargs:\n",
    "                image = processor_step(image, **kwargs)\n",
    "            else:\n",
    "                image = processor_step(image)\n",
    "        return image.astype(np.float32)\n",
    "\n",
    "    def add_processor_step(self, func_name: str, kwargs: dict):\n",
    "        \"\"\"appends preprocessor step from name and kwargs to the chain\n",
    "        args:\n",
    "            func_name: name of the preprocessor function\n",
    "            kwargs: keyword arguments applied to the func_name together with nd image/array\"\"\"\n",
    "        func = getattr(self, func_name)\n",
    "        self._chain.append((func, kwargs))\n",
    "\n",
    "    @classmethod\n",
    "    def to_chain(cls, chain_elems: dict):\n",
    "        \"\"\"adds preprocesser functions to the chain with corresponding elements.\n",
    "        chain_elems contains the functain name as a key and kwargs as values\n",
    "        args:\n",
    "            chain_elems: is a dictionary with the preprocessor function name as key and the coressponding kwargs as\n",
    "            items. Items are a dict itself.\"\"\"\n",
    "        inst = cls()\n",
    "        for chain_elem, elem_kwargs in chain_elems.items():\n",
    "            inst.add_processor_step(chain_elem, elem_kwargs)\n",
    "        return inst\n",
    "\n",
    "    def __call__(self, image: np.ndarray, **kwargs):\n",
    "        \"\"\"executes chain\"\"\"\n",
    "        return self._chain_exe(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cab64d4-6db3-43aa-94bf-95a22111a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePreProcessor(BaseDataProcessor):\n",
    "    \"\"\"Implementation of Abstract Class BaseDataProcessor. Basic steps are implemented for preprocessing.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    \"\"\"all functions take an nd.array as input and return a n.array\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def temp_normalize_image(image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"normalizes images by substracting the mean along temporal axis\"\"\"\n",
    "        return image - np.mean(image, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_image_range(image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"scales image values to an intervall of -1 to 1\"\"\"\n",
    "        return 2 * (image - image.min()) / (image.max() - image.min()) - 1\n",
    "\n",
    "    @staticmethod\n",
    "    def median_normalize_slice(image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"sets the image median to a value of 0\"\"\"\n",
    "        return image - np.median(image, axis=(-2, -1), keepdims=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def set_background_to_one(image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"sets background pixel value to one\"\"\"\n",
    "        if image.sum() / np.prod(image.shape) < 0:\n",
    "            image = image * -1\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a94321db-bddf-4eaa-b2d8-c49a1dda3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor(BasePreProcessor):\n",
    "    \"\"\"implements more specicfic PreProcessor steps, that take additional arguments\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def clip_image(image: np.ndarray, boundaries: Tuple[float, float] = None,\n",
    "                   mode: Literal['multiotsu', 'median'] = None) -> np.ndarray:\n",
    "        \"\"\"clips image values to an intervall based on image intensieties.\n",
    "        Availabe are multiotsu thresholding, and above median\n",
    "        args:\n",
    "            boundaries: image is clipped by those fixed values, must be in the format(min,max). If mode is given,\n",
    "                        boundaries are ignored.\n",
    "            mode: must be either mutlitotsu or median. If multiotsu boundraries are based on two thresholds. If median\n",
    "                  the upper boundary is the median value and lower the min value of the image \"\"\"\n",
    "\n",
    "        if mode == 'multiotsu':\n",
    "            boundaries = threshold_multiotsu(image, classes=3)\n",
    "        elif mode == 'median':\n",
    "            boundaries = [image.min(), None]\n",
    "            boundaries[1] = np.median(image)\n",
    "        elif not boundaries and not mode:\n",
    "            boundaries = [image.min(), image.max()]\n",
    "        return np.clip(image, boundaries[0], boundaries[-1])\n",
    "\n",
    "    @staticmethod\n",
    "    def resample_image(image: np.ndarray, frame_shape=(380, 380)) -> np.ndarray:\n",
    "        \"\"\"resamples image to specified pixel size using nearest neighbours\n",
    "        args:\n",
    "            frame_shape: crops the last two dimensions to the given value\"\"\"\n",
    "        num_frames = len(image)\n",
    "        resized_image = np.zeros((num_frames,) + frame_shape)\n",
    "        for idx, frame in enumerate(image):\n",
    "            resized_image[idx] = cv2.resize(frame, frame_shape, interpolation=cv2.INTER_NEAREST)\n",
    "        return resized_image\n",
    "\n",
    "    @staticmethod\n",
    "    def crop_image(image: np.ndarray, tol=0.01) -> np.ndarray:\n",
    "        \"\"\"removes bars (with noise) on image edges. tol is the amount of noise allowed.\n",
    "            args:\n",
    "                tol: tolerance value above which axis are accepted to contain image information. If a whole row\n",
    "                     contains no information and is at the image edge it is assumed to contain no value for\n",
    "                     classifiation\"\"\"\n",
    "        mask = np.std(image, axis=0) > tol\n",
    "        idx = np.ix_(mask.any(1), mask.any(0))\n",
    "        return image[:, idx[0], idx[1]]\n",
    "\n",
    "    @staticmethod\n",
    "    def center_crop(image, rel_size=(0.5, 0.5)) -> np.ndarray:\n",
    "        \"\"\"takes the center crop of a given image. rel_size is the relative size of the center crop in realtion to the\n",
    "         input image\n",
    "         args:\n",
    "            rel_size: relative size of the center crop in respect to the orginal image. \"\"\"\n",
    "        x_size, y_size = image.shape[-2:]\n",
    "        pos_x_1 = int(x_size * (1 - rel_size[0]) / 2)\n",
    "        pos_y_1 = int(y_size * (1 - rel_size[1]) / 2)\n",
    "        pos_x_2 = int(pos_x_1 + x_size * rel_size[0])\n",
    "        pos_y_2 = int(pos_y_1 + y_size * rel_size[1])\n",
    "        image = image[..., pos_x_1:pos_x_2, pos_y_1:pos_y_2]\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c1a3a31-70f9-4224-9f6c-7359a6135837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, preprocessor_steps: dict):\n",
    "        \"\"\"class to load dicom images from path, gets preprocessor arguments to build the preprocessor\n",
    "        args:\n",
    "            preprocessor_steps: dict containing names and kwargs of the preprocessing steps\"\"\"\n",
    "        self.preprocessor = PreProcessor.to_chain(preprocessor_steps)\n",
    "\n",
    "    def __call__(self, series_paths: List[Union[str, os.PathLike]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"loads images to the RAM and applies the preprocessor steps\n",
    "        args:\n",
    "            series_paths: List of paths. First entry must point to AP and second to the lateral view. Views must be the\n",
    "                          same the length.\n",
    "            return: A tuple containing, two tensors 1.) both views shape: 1 (dummy batch) x time x 2 (views) x height x\n",
    "                    width 2.) length of the series \"\"\"\n",
    "\n",
    "        # loads both dicom views\n",
    "        series = [self._load_image(path) for path in series_paths]\n",
    "        series_lengths = [len(view) for view in series]\n",
    "        assert len(series_lengths) == 2, f'expected 2 views, got {len(series_lengths)} views'\n",
    "        assert series_lengths[0] == series_lengths[1], f'Length of DSA views are not equal'\n",
    "\n",
    "        # converts views to array and make a 3Ch image. The 3Ch are filled with consecutive frames\n",
    "        series = np.array(series)\n",
    "        series = self._make_3_ch_img(series)\n",
    "        series = torch.tensor(series, dtype=torch.float32).unsqueeze(dim=0)\n",
    "        return series, torch.tensor(series_lengths[0]).unsqueeze(dim=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_3_ch_img(img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"creates a 3Ch image from a 1Ch image. The 3Ch consists of 3 consecutive frames, centered around the original\n",
    "        Frame\n",
    "        arg:\n",
    "            img: 1CH Image\n",
    "        return: same image with 3CH. Each slice from the orginal image is converted to the center channel sourounded by\n",
    "                the previous and nect slice.\"\"\"\n",
    "        new_img = np.zeros((*img.shape[0:2], 3, *img.shape[-2:])).swapaxes(1, 0)\n",
    "        for i_frame in range(len(new_img)):\n",
    "            img_temp = np.zeros(new_img.shape[1:])\n",
    "            for i_channel in range(img_temp.shape[1]):\n",
    "                if i_frame == 0 and i_channel == 0:\n",
    "                    pass\n",
    "                elif i_frame == len(new_img) - 1 and i_channel == 2:\n",
    "                    pass\n",
    "                else:\n",
    "                    img_temp[:, i_channel] = img[:, i_frame - 1 + i_channel]\n",
    "            new_img[i_frame] = img_temp\n",
    "        return new_img\n",
    "\n",
    "    def _load_image(self, image_path: Union[str, os.PathLike]) -> np.ndarray:\n",
    "        \"\"\"loads dicom images to RAM and preprocesses image from path\n",
    "            args:\n",
    "                image_paths: list containing path to two views order: [ap, lat]\n",
    "            return: preprocessed dicom image as np.array\"\"\"\n",
    "        image = self._load_img_to_ram(image_path)\n",
    "        image = self.preprocessor(image)\n",
    "        return image.astype(np.half)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_img_to_ram(file_path: Union[str, os.PathLike]) -> np.ndarray:\n",
    "        \"\"\"loads dicom image to RAM\n",
    "            args:\n",
    "                image_path: path pointing to an dicom image\n",
    "            return: dicom image as np.array\"\"\"\n",
    "        assert file_path.endswith('.dcm'), 'only Dicom file format is supported'\n",
    "        image = sitk.GetArrayFromImage(sitk.ReadImage(file_path))\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028f5b0-61de-444e-9664-fe02ed77cae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8c60a7b-2c66-4ac1-a2e9-4b0b0da7878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'pretrained': 'model_weights/experiment_I', \n",
    "    'feature_size': 1280, \n",
    "    'num_classes': 6, \n",
    "    'in_channels': 3, \n",
    "    'output_size': 1280\n",
    "}\n",
    "model = TICIModelHandler(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2b0f22e-5b0b-4945-9dd0-aff80b712eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_steps = {\n",
    "    'crop_image': {'tol': 0.01}, \n",
    "    'clip_image': {'mode': 'multiotsu'}, \n",
    "    'temp_normalize_image': None, \n",
    "    'normalize_image_range': None, \n",
    "    'resample_image': {\n",
    "        'frame_shape': (224, 224)\n",
    "    }\n",
    "}\n",
    "data_loader = DataLoader(preprocessor_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d6e8507-15b0-4e57-8af4-6efe24ff7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcm_path = [\n",
    "    'D:\\\\Xuchu_Liu\\\\Workspace\\\\DataSet\\\\DiveMed\\\\DB_images\\\\Rush\\\\1.2.392.200036.9116.3.1.12582476.20230927141610851.1011.7.7724_0002_210000_16981758460067.dcm',\n",
    "    'D:\\\\Xuchu_Liu\\\\Workspace\\\\DataSet\\\\DiveMed\\\\DB_images\\\\Rush\\\\1.2.392.200036.9116.3.1.12582476.20230927141610851.1011.7.7724_0002_220000_16981758470068.dcm'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dbafca-8c62-4108-85db-a97e383359b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a99e9bc-f612-47ab-a113-184b024170b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0038, 0.3732, 0.5921, 0.0292, 0.0007, 0.0009]])\n",
      "[2.0]\n",
      "TICI 2a score 2\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {\n",
    "    0: 'TICI 0 score 0', \n",
    "    1: 'TICI 0 score 1', \n",
    "    2: 'TICI 2a score 2', \n",
    "    3: 'TICI 2b score 3', \n",
    "    4: 'TICI 3 score 4', \n",
    "    5: 'TICI 3 score 5'\n",
    "}\n",
    "\n",
    "data = data_loader(dcm_path)\n",
    "raw_values = model(*data, model_mode=ModelMode.inference, output_mode=OutputMode.last_frame)\n",
    "print(raw_values)\n",
    "score = [float(torch.argmax(raw_value, dim=-1)) for raw_value in raw_values]\n",
    "print(score)\n",
    "score = score[0]\n",
    "print(label_mapping[score])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f170d18-1174-41f2-ae4f-c134c13d8f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d899e7-2779-4c28-bf37-01e6471c7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L0MKNDE0-e2o",
   "metadata": {
    "id": "L0MKNDE0-e2o"
   },
   "source": [
    "## Clean up temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a32d1-b747-47d4-81a5-f834fb1c94c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
